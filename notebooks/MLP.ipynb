{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772de206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š MLP COMPLET - GRID SEARCH EXHAUSTIF (MODE COMPLET)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# Description: MLP avec TOUS les hyperparamÃ¨tres Ã  tuner - MODE COMPLET\n",
    "# - Teste TOUTES les combinaisons possibles\n",
    "# - Charge donnÃ©es FAMD dÃ©jÃ  faites\n",
    "# - Early stopping (ON/OFF)\n",
    "# - DiffÃ©rentes activations (ReLU, Sigmoid, Tanh)\n",
    "# - Dropout\n",
    "# - Optimizers (SGD, Momentum, Adam)\n",
    "# Date: 2025-12-06\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1ï¸âƒ£  IMPORTATIONS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2ï¸âƒ£  âš™ï¸  HYPERPARAMÃˆTRES Ã€ TESTER - MODE COMPLET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"âš™ï¸  PARAMÃˆTRES Ã€ TUNER - MODE COMPLET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "# ARCHITECTURE\n",
    "HIDDEN_SIZES_OPTIONS = [\n",
    "    [32], [64], [128], [256],              # 1 couche\n",
    "    [64, 32], [128, 64]      # 2 couches\n",
    "]\n",
    "\n",
    "# APPRENTISSAGE\n",
    "LEARNING_RATE_OPTIONS = [0.001, 0.01, 0.05]\n",
    "\n",
    "EPOCHS_OPTIONS = [200, 500, 1000]\n",
    "\n",
    "BATCH_SIZE_OPTIONS = [32]\n",
    "\n",
    "# RÃ‰GULARISATION\n",
    "REG_LAMBDA_OPTIONS = [0.0, 0.01]\n",
    "\n",
    "DROPOUT_OPTIONS = [0.0, 0.3]  # Dropout rate\n",
    "\n",
    "# FONCTIONS D'ACTIVATION\n",
    "ACTIVATION_OPTIONS = ['relu', 'tanh']\n",
    "\n",
    "# EARLY STOPPING\n",
    "EARLY_STOPPING_OPTIONS = [False, True]\n",
    "\n",
    "# OPTIMIZER\n",
    "OPTIMIZER_OPTIONS = ['sgd', 'adam']\n",
    "\n",
    "\n",
    "# CALCUL DU TOTAL\n",
    "total = (len(HIDDEN_SIZES_OPTIONS) * len(LEARNING_RATE_OPTIONS) * \n",
    "         len(EPOCHS_OPTIONS) * len(BATCH_SIZE_OPTIONS) * len(REG_LAMBDA_OPTIONS) *\n",
    "         len(DROPOUT_OPTIONS) * len(ACTIVATION_OPTIONS) * len(EARLY_STOPPING_OPTIONS) *\n",
    "         len(OPTIMIZER_OPTIONS))\n",
    "\n",
    "print(f\"\\nâœ… Architectures: {len(HIDDEN_SIZES_OPTIONS)}\")\n",
    "print(f\"âœ… Learning rates: {len(LEARNING_RATE_OPTIONS)}\")\n",
    "print(f\"âœ… Epochs: {len(EPOCHS_OPTIONS)}\")\n",
    "print(f\"âœ… Batch sizes: {len(BATCH_SIZE_OPTIONS)}\")\n",
    "print(f\"âœ… Regularizations: {len(REG_LAMBDA_OPTIONS)}\")\n",
    "print(f\"âœ… Dropout rates: {len(DROPOUT_OPTIONS)}\")\n",
    "print(f\"âœ… Activations: {len(ACTIVATION_OPTIONS)}\")\n",
    "print(f\"âœ… Early Stopping: {len(EARLY_STOPPING_OPTIONS)}\")\n",
    "print(f\"âœ… Optimizers: {len(OPTIMIZER_OPTIONS)}\")\n",
    "print(f\"\\nğŸ“Š TOTAL DE COMBINAISONS: {total:,}\")\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3ï¸âƒ£  CLASSE MLP AVEC TOUTES LES FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, learning_rate=0.01, \n",
    "                 reg_lambda=0.0, dropout_rate=0.0, activation='relu', optimizer='sgd'):\n",
    "        \n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Initialiser poids pour toutes les couches\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * np.sqrt(2/layer_sizes[i])\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Momentum pour optimizer\n",
    "        if optimizer == 'momentum':\n",
    "            self.momentum_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.momentum_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.momentum_beta = 0.9\n",
    "        \n",
    "        # Adam pour optimizer\n",
    "        if optimizer == 'adam':\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "            self.beta1 = 0.9\n",
    "            self.beta2 = 0.999\n",
    "            self.epsilon = 1e-8\n",
    "    \n",
    "    \n",
    "    def _activation(self, x):\n",
    "        \"\"\"Fonction d'activation\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "    \n",
    "    \n",
    "    def _activation_derivative(self, a, z):\n",
    "        \"\"\"DÃ©rivÃ©e d'activation\"\"\"\n",
    "        if self.activation == 'relu':\n",
    "            return (z > 0).astype(float)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == 'tanh':\n",
    "            return 1 - a**2\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"Softmax\"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, X, training=False):\n",
    "        \"\"\"Forward pass avec dropout\"\"\"\n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        self.dropout_masks = []\n",
    "        \n",
    "        # Couches cachÃ©es\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
    "            self.z_values.append(z)\n",
    "            a = self._activation(z)\n",
    "            \n",
    "            # Dropout\n",
    "            if training and self.dropout_rate > 0:\n",
    "                mask = np.random.binomial(1, 1 - self.dropout_rate, a.shape) / (1 - self.dropout_rate)\n",
    "                a = a * mask\n",
    "                self.dropout_masks.append(mask)\n",
    "            else:\n",
    "                self.dropout_masks.append(None)\n",
    "            \n",
    "            self.activations.append(a)\n",
    "        \n",
    "        # Couche sortie (softmax)\n",
    "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        self.z_values.append(z)\n",
    "        a = self.softmax(z)\n",
    "        self.activations.append(a)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        \"\"\"Backward pass\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        y_one_hot = np.eye(self.output_size)[y]\n",
    "        \n",
    "        # Gradient sortie\n",
    "        dz = self.activations[-1] - y_one_hot\n",
    "        \n",
    "        d_weights = []\n",
    "        d_biases = []\n",
    "        \n",
    "        # Couche sortie\n",
    "        d_weights.append(np.dot(self.activations[-2].T, dz) / n_samples)\n",
    "        d_biases.append(np.sum(dz, axis=0, keepdims=True) / n_samples)\n",
    "        \n",
    "        # Couches cachÃ©es\n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dz = np.dot(dz, self.weights[i+1].T) * self._activation_derivative(self.activations[i+1], self.z_values[i])\n",
    "            \n",
    "            # Dropout backward\n",
    "            if self.dropout_masks[i] is not None:\n",
    "                dz = dz * self.dropout_masks[i]\n",
    "            \n",
    "            if i == 0:\n",
    "                d_weights.insert(0, np.dot(X.T, dz) / n_samples)\n",
    "            else:\n",
    "                d_weights.insert(0, np.dot(self.activations[i].T, dz) / n_samples)\n",
    "            d_biases.insert(0, np.sum(dz, axis=0, keepdims=True) / n_samples)\n",
    "        \n",
    "        # RÃ©gularisation L2\n",
    "        for i in range(len(d_weights)):\n",
    "            d_weights[i] += (self.reg_lambda / n_samples) * self.weights[i]\n",
    "        \n",
    "        return d_weights, d_biases\n",
    "    \n",
    "    \n",
    "    def update_weights_sgd(self, d_weights, d_biases):\n",
    "        \"\"\"SGD simple\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * d_weights[i]\n",
    "            self.biases[i] -= self.learning_rate * d_biases[i]\n",
    "    \n",
    "    \n",
    "    def update_weights_momentum(self, d_weights, d_biases):\n",
    "        \"\"\"SGD avec momentum\"\"\"\n",
    "        for i in range(len(self.weights)):\n",
    "            self.momentum_w[i] = self.momentum_beta * self.momentum_w[i] - self.learning_rate * d_weights[i]\n",
    "            self.momentum_b[i] = self.momentum_beta * self.momentum_b[i] - self.learning_rate * d_biases[i]\n",
    "            \n",
    "            self.weights[i] += self.momentum_w[i]\n",
    "            self.biases[i] += self.momentum_b[i]\n",
    "    \n",
    "    \n",
    "    def update_weights_adam(self, d_weights, d_biases):\n",
    "        \"\"\"Adam optimizer\"\"\"\n",
    "        self.t += 1\n",
    "        \n",
    "        for i in range(len(self.weights)):\n",
    "            # Weights\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * d_weights[i]\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (d_weights[i]**2)\n",
    "            \n",
    "            m_hat_w = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "            v_hat_w = self.v_w[i] / (1 - self.beta2**self.t)\n",
    "            \n",
    "            self.weights[i] -= self.learning_rate * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "            \n",
    "            # Biases\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * d_biases[i]\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (d_biases[i]**2)\n",
    "            \n",
    "            m_hat_b = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "            v_hat_b = self.v_b[i] / (1 - self.beta2**self.t)\n",
    "            \n",
    "            self.biases[i] -= self.learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, y_pred, y):\n",
    "        \"\"\"Cross-entropy loss\"\"\"\n",
    "        n_samples = y_pred.shape[0]\n",
    "        y_one_hot = np.eye(self.output_size)[y]\n",
    "        \n",
    "        loss = -np.mean(np.log(y_pred + 1e-15) * y_one_hot)\n",
    "        \n",
    "        reg_loss = (self.reg_lambda / (2 * n_samples)) * sum(np.sum(w**2) for w in self.weights)\n",
    "        \n",
    "        return loss + reg_loss\n",
    "    \n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs=500, batch_size=32, early_stopping=False, patience=20):\n",
    "        \"\"\"EntraÃ®nement avec early stopping optionnel\"\"\"\n",
    "        train_loss_hist = []\n",
    "        val_loss_hist = []\n",
    "        best_val_loss = np.inf\n",
    "        patience_counter = 0\n",
    "        epochs_actual = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Mini-batch\n",
    "            indices = np.random.permutation(len(X_train))[:batch_size]\n",
    "            X_batch = X_train[indices]\n",
    "            y_batch = y_train[indices]\n",
    "            \n",
    "            # Forward + Backward\n",
    "            y_pred = self.forward(X_batch, training=True)\n",
    "            train_loss = self.compute_loss(y_pred, y_batch)\n",
    "            train_loss_hist.append(train_loss)\n",
    "            \n",
    "            d_weights, d_biases = self.backward(X_batch, y_batch)\n",
    "            \n",
    "            # Update selon optimizer\n",
    "            if self.optimizer == 'sgd':\n",
    "                self.update_weights_sgd(d_weights, d_biases)\n",
    "            elif self.optimizer == 'momentum':\n",
    "                self.update_weights_momentum(d_weights, d_biases)\n",
    "            elif self.optimizer == 'adam':\n",
    "                self.update_weights_adam(d_weights, d_biases)\n",
    "            \n",
    "            epochs_actual += 1\n",
    "            \n",
    "            # Validation loss (early stopping)\n",
    "            if early_stopping:\n",
    "                y_pred_val = self.forward(X_val, training=False)\n",
    "                val_loss = self.compute_loss(y_pred_val, y_val)\n",
    "                val_loss_hist.append(val_loss)\n",
    "                \n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "        \n",
    "        return train_loss_hist, val_loss_hist, epochs_actual\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"PrÃ©dictions\"\"\"\n",
    "        output = self.forward(X, training=False)\n",
    "        return np.argmax(output, axis=1)\n",
    "    \n",
    "    \n",
    "    def evaluate(self, X, y):\n",
    "        \"\"\"Ã‰valuation\"\"\"\n",
    "        y_pred = self.forward(X, training=False)\n",
    "        y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "        accuracy = accuracy_score(y, y_pred_labels)\n",
    "        loss = self.compute_loss(y_pred, y)\n",
    "        return accuracy, loss\n",
    "\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4ï¸âƒ£  CHARGEMENT DONNÃ‰ES DÃ‰JÃ€ PRÃ‰PARÃ‰ES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š CHARGEMENT DES DONNÃ‰ES (DÃ‰JÃ€ FAITES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Charger donnÃ©es dÃ©jÃ  faites\n",
    "df_train = pd.read_csv(Path.cwd() / 'data_train_encoded.csv')\n",
    "X_train = df_train.drop('Target', axis=1).values  # Tout sauf Target\n",
    "y_train = df_train['Target'].values               # Que Target\n",
    "\n",
    "# Charger donnÃ©es dÃ©jÃ  faites\n",
    "df_test = pd.read_csv(Path.cwd() / 'data_test_encoded.csv')\n",
    "X_test = df_test.drop('Target', axis=1).values  # Tout sauf Target\n",
    "y_test = df_test['Target'].values               # Que Target\n",
    "\n",
    "# Split validation (20% du train)\n",
    "split_idx = int(0.8 * len(X_train))\n",
    "X_train_split = X_train[:split_idx]\n",
    "y_train_split = y_train[:split_idx]\n",
    "X_val = X_train[split_idx:]\n",
    "y_val = y_train[split_idx:]\n",
    "\n",
    "print(f\"âœ… Train: {X_train_split.shape}\")\n",
    "print(f\"âœ… Val:   {X_val.shape}\")\n",
    "print(f\"âœ… Test:  {X_test.shape}\")\n",
    "print(f\"âœ… Classes: {len(np.unique(y_train_split))}\")\n",
    "\n",
    "\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5ï¸âƒ£  GRID SEARCH COMPLET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ” GRID SEARCH EXHAUSTIF - EN COURS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "current = 1\n",
    "\n",
    "for hidden_sizes in HIDDEN_SIZES_OPTIONS:\n",
    "    for lr in LEARNING_RATE_OPTIONS:\n",
    "        for epochs in EPOCHS_OPTIONS:\n",
    "            for batch_size in BATCH_SIZE_OPTIONS:\n",
    "                for reg_lambda in REG_LAMBDA_OPTIONS:\n",
    "                    for dropout in DROPOUT_OPTIONS:\n",
    "                        for activation in ACTIVATION_OPTIONS:\n",
    "                            for early_stopping in EARLY_STOPPING_OPTIONS:\n",
    "                                for optimizer in OPTIMIZER_OPTIONS:\n",
    "                                    \n",
    "                                    # CrÃ©er MLP\n",
    "                                    mlp = MLP(\n",
    "                                        input_size=X_train_split.shape[1],\n",
    "                                        hidden_sizes=hidden_sizes,\n",
    "                                        output_size=len(np.unique(y_train_split)),\n",
    "                                        learning_rate=lr,\n",
    "                                        reg_lambda=reg_lambda,\n",
    "                                        dropout_rate=dropout,\n",
    "                                        activation=activation,\n",
    "                                        optimizer=optimizer\n",
    "                                    )\n",
    "                                    \n",
    "                                    # EntraÃ®ner\n",
    "                                    _, _, epochs_actual = mlp.train(X_train_split, y_train_split, X_val, y_val, \n",
    "                                                                    epochs=epochs, batch_size=batch_size, \n",
    "                                                                    early_stopping=early_stopping, patience=20)\n",
    "                                    \n",
    "                                    # Ã‰valuer\n",
    "                                    train_acc, train_loss = mlp.evaluate(X_train_split, y_train_split)\n",
    "                                    test_acc, test_loss = mlp.evaluate(X_test, y_test)\n",
    "                                    \n",
    "                                    results.append({\n",
    "                                        'hidden_sizes': str(hidden_sizes),\n",
    "                                        'lr': lr,\n",
    "                                        'epochs': epochs,\n",
    "                                        'batch_size': batch_size,\n",
    "                                        'reg_lambda': reg_lambda,\n",
    "                                        'dropout': dropout,\n",
    "                                        'activation': activation,\n",
    "                                        'early_stopping': early_stopping,\n",
    "                                        'optimizer': optimizer,\n",
    "                                        'epochs_actual': epochs_actual,\n",
    "                                        'train_acc': train_acc,\n",
    "                                        'train_loss': train_loss,\n",
    "                                        'test_acc': test_acc,\n",
    "                                        'test_loss': test_loss\n",
    "                                    })\n",
    "                                    \n",
    "                                    overfit = train_acc - test_acc\n",
    "                                    \n",
    "                                    print(f\"[{current:,}/{total:,}] {str(hidden_sizes):20s} | \"\n",
    "                                          f\"{activation:6s} | LR:{lr:.5f} | Drop:{dropout:.1f} | \"\n",
    "                                          f\"ES:{str(early_stopping):5s} | Opt:{optimizer:8s} | \"\n",
    "                                          f\"Train:{train_acc:.2%} | Test:{test_acc:.2%} | \"\n",
    "                                          f\"Overfit:{overfit:+.2%}\")\n",
    "                                    \n",
    "                                    current += 1\n",
    "\n",
    "# Sauvegarder rÃ©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('test_acc', ascending=False)\n",
    "\n",
    "graphique_folder = Path.cwd() / 'graphiques'\n",
    "graphique_folder.mkdir(exist_ok=True)\n",
    "\n",
    "results_df.to_csv(graphique_folder / 'mlp_grid_search_results_complete.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ† TOP 15 MEILLEURES COMBINAISONS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.head(15).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âš ï¸  TOP 15 PIRES COMBINAISONS (OVERFITTING)\")\n",
    "print(\"=\"*80)\n",
    "results_df['overfit'] = results_df['train_acc'] - results_df['test_acc']\n",
    "print(results_df.nlargest(15, 'overfit')[['hidden_sizes', 'activation', 'lr', 'dropout', 'optimizer', 'train_acc', 'test_acc', 'overfit']].to_string(index=False))\n",
    "\n",
    "# Meilleurs params\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"âœ… MEILLEURE COMBINAISON TROUVÃ‰E\")\n",
    "print(\"=\"*80)\n",
    "for col in ['hidden_sizes', 'lr', 'epochs', 'batch_size', 'reg_lambda', 'dropout', 'activation', 'early_stopping', 'optimizer', 'epochs_actual', 'train_acc', 'test_acc']:\n",
    "    print(f\"  {col:20s}: {best[col]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š STATISTIQUES\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Meilleure accuracy test:   {results_df['test_acc'].max():.2%}\")\n",
    "print(f\"âœ… Pire accuracy test:        {results_df['test_acc'].min():.2%}\")\n",
    "print(f\"âœ… Moyenne accuracy test:     {results_df['test_acc'].mean():.2%}\")\n",
    "print(f\"âœ… Meilleur overfitting:      {results_df['overfit'].min():+.2%}\")\n",
    "print(f\"âœ… Pire overfitting:          {results_df['overfit'].max():+.2%}\")\n",
    "\n",
    "print(f\"\\nâœ… RÃ©sultats sauvegardÃ©s dans: graphiques/mlp_grid_search_results_complete.csv\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
