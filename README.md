# ğŸš€ DropOutGuard

> PrÃ©diction intelligente de l'Ã©chec scolaire. DÃ©tection prÃ©coce des Ã©tudiants Ã  risque via Deep Learning.

[![Python](https://img.shields.io/badge/Python-3.8+-3776ab?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

---

## ğŸ¯ Vision

DropOutGuard combine **analyse factorielle de donnÃ©es mixtes (AFDM)** et **rÃ©seaux de neurones MLP** pour identifier prÃ©cocement les Ã©tudiants en risque de dÃ©crochage scolaire.

Une solution IA pour **prÃ©dire, analyser et intervenir**.

## ğŸ§  Concepts clÃ©s

- **Propagation avant** : traversÃ©e du rÃ©seau input â†’ output
- **Backpropagation** : calcul des gradients via chaÃ®ne de dÃ©rivÃ©es
- **Vanishing Gradients** : saturation tanh/sigmoid en profondeur
- **ReLU / Tanh / Sigmoid** : fonctions d'activation non-linÃ©aires
- **Cross-Entropy Loss** : minimisation de l'erreur classification
- **Descente de gradient** : optimisation des poids

## ğŸ›  Stack

| Composant | Tech |
|-----------|------|
| **Deep Learning** | PyTorch 2.0+ |
| **PrÃ©processing** | AFDM (scikit-learn / rpy2) |
| **Data** | Pandas, NumPy |
| **Viz** | Matplotlib, Seaborn |

## ğŸ“‚ Architecture

```
dropout-guard/
â”œâ”€â”€ ğŸ“Š data/
â”‚   â””â”€â”€ etudiants.csv
â”œâ”€â”€ ğŸ”§ src/
â”‚   â”œâ”€â”€ preprocess.py      â† AFDM engine
â”‚   â”œâ”€â”€ model.py           â† MLP architecture
â”‚   â”œâ”€â”€ train.py           â† Training loop
â”‚   â””â”€â”€ evaluate.py        â† Metrics & plots
â”œâ”€â”€ ğŸ““ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb       â† Data exploration
â”‚   â””â”€â”€ 02_results.ipynb   â† Analysis & insights
â”œâ”€â”€ ğŸ“ˆ results/            â† Models & visualizations
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš¡ Quick Start

```bash
# Clone & setup
git clone https://github.com/tonpseudo/dropout-guard.git
cd dropout-guard

# Install deps
pip install -r requirements.txt

# Train model
python src/train.py --data data/etudiants.csv --epochs 100 --batch-size 32
```

## ğŸ“Š Performance

| MÃ©trique | Baseline | With AFDM | Gain â†‘ |
|----------|:--------:|:---------:|:------:|
| Accuracy | 78% | **88%** | +10% |
| F1-Score | 0.75 | **0.86** | +0.11 |
| AUC-ROC | 0.82 | **0.91** | +0.09 |

âœ… **~85-90% accuracy** en 5-fold cross-validation

## ğŸ“‹ Dataset

**Variables quantitatives :**
- Notes prÃ©cÃ©dentes | Absences | Heures d'Ã©tude | GPA

**Variables qualitatives :**
- FiliÃ¨re | Niveau socio-Ã©ducatif | Genre | Situation emploi

## ğŸ“ Learning Goals

- âœ… ImplÃ©menter une AFDM from scratch (sans prince)
- âœ… Construire un MLP PyTorch personnalisÃ©
- âœ… MaÃ®triser forward/backward propagation
- âœ… GÃ©rer donnÃ©es mixtes quantitatives + catÃ©gorielles
- âœ… Analyser et interprÃ©ter rÃ©sultats

## ğŸ§© Concepts couverts

- **Propagation avant / Backpropagation**
- **Fonctions d'activation** : ReLU, Tanh, Sigmoid
- **Loss functions** : Cross-Entropy
- **Optimizers** : SGD, Adam
- **Vanishing Gradients** & mitigation
- **Regularization** : Dropout, L2
- **Validation croisÃ©e** : k-fold strategy

## ğŸ“¦ Dependencies

```
torch>=2.0.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
jupyter>=1.0.0
rpy2>=3.5.0          # Optional: FactoMineR integration
```

## ğŸ¤ Contributing

```bash
git checkout -b feature/your-feature
git commit -m "âœ¨ Add cool feature"
git push origin feature/your-feature
```

## ğŸ“„ License

MIT License - voir [LICENSE](LICENSE)

---

**Master IA & Data** â€¢ La Plateforme_ â€¢ Marseille  
ğŸ”¬ *DÃ©tection intelligente â€¢ Intervention prÃ©coce â€¢ Impact rÃ©el*
